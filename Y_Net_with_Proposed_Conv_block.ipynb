{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Y-net with new convolution block for depth estimation\n",
    "\n",
    "### This notebook contains code to run a new depth estimation model called Y-net with a new convolutional block\n",
    "Done By:\n",
    "Chandravaran Kunjeti\n",
    "Saikumar Dande"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iKdJJXQgM5U1"
   },
   "outputs": [],
   "source": [
    "!pip install albumentations==0.4.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AjhntKacJO7s",
    "outputId": "67780beb-94ce-4cfa-f196-30762446d941"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l8QDh0z9OsBN",
    "outputId": "d7f57a0a-ecea-4ad3-ad1f-9f8ad0577f41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/Neural Network Project\n"
     ]
    }
   ],
   "source": [
    "%cd /content/drive/MyDrive/Neural\\ Network\\ Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lvIlIPM5Nbrx"
   },
   "outputs": [],
   "source": [
    "from DataLoader import TransposeDepthInput, NYUDataset, save_checkpoint, get_loaders, save_predictions_as_imgs\n",
    "from metrics import ScaleInvariantLoss, threeshold_percentage, rmse_linear, rmse_log, abs_relative_difference, squared_relative_difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iuTQGWdupqnt"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size = 3):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, padding = 'same', bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size, padding = 'same', bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.conv1_1 = nn.Conv2d(in_channels, out_channels, kernel_size = 1, padding = 'same', bias=False)\n",
    "        self.conv3_1 = nn.Conv2d(in_channels, out_channels, kernel_size = 3, padding = 'same', bias=False)\n",
    "        self.conv5_1 = nn.Conv2d(in_channels, out_channels, kernel_size = 5, padding = 'same', bias=False)\n",
    "        self.conv3_2 = nn.Conv2d(3*out_channels, out_channels, kernel_size = 3, padding = 'same', bias=False)\n",
    "\n",
    "        self.batchNorm1 = nn.BatchNorm2d(3*out_channels)\n",
    "        self.batchNorm2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.cat((self.conv1_1(x), self.conv3_1(x), self.conv5_1(x)), dim=1)\n",
    "        x = self.relu1(self.batchNorm1(x))\n",
    "        x = self.batchNorm2(self.conv3_2(x))\n",
    "        return self.relu2(x)\n",
    "\n",
    "class DownConv(nn.Module):\n",
    "    def __init__(self, in_channels, features=[64, 128, 256, 512]):\n",
    "        super(DownConv, self).__init__()\n",
    "        self.downs = nn.ModuleList()\n",
    "        self.residualBlocks = nn.ModuleList()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Down part of UNET\n",
    "        for feature in features:\n",
    "            self.downs.append(DoubleConv(in_channels, feature))\n",
    "            self.residualBlocks.append(ResidualBlock(feature, feature, kernel_size = 5))\n",
    "            in_channels = feature\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.skip_connections = []\n",
    "\n",
    "        for i in range(len(self.downs)):\n",
    "            x = self.downs[i](x)\n",
    "            skip_connection = self.residualBlocks[i](x)\n",
    "            self.skip_connections.append(skip_connection)\n",
    "            x = self.pool(x)\n",
    "        \n",
    "        self.skip_connections = self.skip_connections[::-1]\n",
    "        return x\n",
    "\n",
    "class UpConv(nn.Module):\n",
    "    def __init__(self, out_channels, downConvs1, downConvs2, features=[512, 256, 128, 64]):\n",
    "        super(UpConv, self).__init__()\n",
    "        self.ups = nn.ModuleList()\n",
    "        self.downConvs1 = downConvs1\n",
    "        self.downConvs2 = downConvs2\n",
    "\n",
    "        # Up part of UNET\n",
    "        for feature in features:\n",
    "            self.ups.append(\n",
    "                nn.ConvTranspose2d(\n",
    "                    feature, feature, kernel_size=2, stride=2,\n",
    "                )\n",
    "            )\n",
    "            self.ups.append(DoubleConv(feature*3, feature//2))\n",
    "      \n",
    "    def forward(self, x):\n",
    "        for idx in range(0, len(self.ups), 2):\n",
    "            x = self.ups[idx](x)\n",
    "            skip_connection1 = self.downConvs1.skip_connections[idx//2]\n",
    "            skip_connection2 = self.downConvs2.skip_connections[idx//2]\n",
    "\n",
    "            if x.shape != skip_connection1.shape:\n",
    "                x = TF.resize(x, size=skip_connection1.shape[2:])\n",
    "\n",
    "            concat_skip = torch.cat((skip_connection1, skip_connection2, x), dim=1)\n",
    "            x = self.ups[idx+1](concat_skip)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "# In this network, we send the RGB and segmentation input separately\n",
    "class YNET(nn.Module):\n",
    "    def __init__(\n",
    "            self, in_channels1=3, in_channels2=2, out_channels=1, features=[64, 128, 256, 512],\n",
    "    ):\n",
    "        super(YNET, self).__init__()\n",
    "        self.ups = nn.ModuleList()\n",
    "        self.downConvs1 = DownConv(in_channels1, features)\n",
    "        self.downConvs2 = DownConv(in_channels2, features)\n",
    "        self.bottleneck = DoubleConv(features[-1]*2, features[-1])\n",
    "        self.upConvs = UpConv(out_channels, self.downConvs1, self.downConvs2, reversed(features))\n",
    "\n",
    "        self.final_conv = nn.Conv2d(features[0]//2, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, image, gradient):\n",
    "        x1 = self.downConvs1(image)\n",
    "        x2 = self.downConvs2(gradient)\n",
    "\n",
    "        x = self.bottleneck(torch.cat((x1, x2), dim=1))\n",
    "        x = self.upConvs(x)\n",
    "\n",
    "        return self.final_conv(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-etyVshYi_on",
    "outputId": "4a828544-c919-418e-e8a4-2e733469bd2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape\t: torch.Size([3, 3, 120, 160])\n",
      "Gradient shape\t: torch.Size([3, 2, 120, 160])\n",
      "Output shape\t: torch.Size([3, 1, 120, 160])\n"
     ]
    }
   ],
   "source": [
    "def test():\n",
    "    image = torch.randn((3, 3, 120, 160))\n",
    "    gradient = torch.randn((3, 2, 120, 160))\n",
    "    model = YNET(in_channels1=3, in_channels2=2, out_channels=1)\n",
    "    preds = model(image, gradient)\n",
    "    print(\"Input shape\\t:\", image.shape)\n",
    "    print(\"Gradient shape\\t:\", gradient.shape)\n",
    "    print(\"Output shape\\t:\", preds.shape)\n",
    "    assert preds.shape[2:] == image.shape[2:]\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KMMxawVqV_qM"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "IMAGE_HEIGHT = 120\n",
    "IMAGE_WIDTH = 160\n",
    "\n",
    "rgb_data_transforms_1 = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_HEIGHT, IMAGE_WIDTH)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "def Save_Predictions(model, file_names, image_dir, depth_dir, save_dir):\n",
    "    model.eval()\n",
    "    for image_name in file_names:\n",
    "        # Load the image and dpeth\n",
    "        image = cv2.imread(image_dir + image_name, cv2.IMREAD_UNCHANGED)\n",
    "        depth = cv2.imread(depth_dir+ image_name, cv2.IMREAD_UNCHANGED)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = Image.fromarray(image)\n",
    "        image = rgb_data_transforms_1(image)\n",
    "\n",
    "        # Find the gradient\n",
    "        gray = np.moveaxis(image.numpy(), [0, 1, 2], [2, 0, 1])\n",
    "        gray = cv2.cvtColor(gray, cv2.COLOR_BGR2GRAY)\n",
    "        gx = cv2.Sobel(gray, ddepth = cv2.CV_32F, dx=1, dy=0, ksize=3)\n",
    "        gy = cv2.Sobel(gray, ddepth = cv2.CV_32F, dx=0, dy=1, ksize=3)\n",
    "        gradient = torch.from_numpy(np.stack([gx, gy]))\n",
    "\n",
    "        image = torch.unsqueeze(image, 0)\n",
    "        gradient = torch.unsqueeze(gradient, 0)\n",
    "\n",
    "        # Predict the output\n",
    "        image = image.to(device=DEVICE)\n",
    "        gradient = gradient.to(device=DEVICE)\n",
    "        with torch.no_grad():\n",
    "            predicted = model(image, gradient)\n",
    "\n",
    "        image = image.cpu()\n",
    "        predicted = predicted.cpu()\n",
    "\n",
    "        input_image = np.zeros((120, 160, 3), dtype=np.float32)\n",
    "        input_image[:, :, 0] = image[0, 0, :, :]\n",
    "        input_image[:, :, 1] = image[0, 1, :, :]\n",
    "        input_image[:, :, 2] = image[0, 2, :, :]\n",
    "        predicted = predicted[0, 0, :, :]\n",
    "\n",
    "        fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "        ax = fig.add_subplot(1, 3, 1)\n",
    "        ax.set_title('Input image')\n",
    "        plt.imshow(input_image)\n",
    "        ax = fig.add_subplot(1, 3, 2)\n",
    "        ax.set_title('Ground truth')\n",
    "        plt.imshow(depth, cmap='gist_gray')    #plt.imshow(actual_depth, cmap='jet')\n",
    "        ax = fig.add_subplot(1, 3, 3)\n",
    "        ax.set_title('Ynet + New Block predicted') \n",
    "        plt.imshow(predicted, cmap='gist_gray')\n",
    "        plt.savefig(f'{save_dir}/{image_name}')\n",
    "        plt.close(fig)\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3f_UsHLIoOAG"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Hyperparameters etc.\n",
    "LEARNING_RATE = 1e-4\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 100\n",
    "NUM_WORKERS = 16\n",
    "PIN_MEMORY = True\n",
    "LOAD_MODEL = True\n",
    "TRAIN_IMG_DIR = \"Datasets/Train/images/\"\n",
    "TRAIN_DEPTH_DIR = \"Datasets/Train/depths/\"\n",
    "VAL_IMG_DIR = \"Datasets/Validation/images/\"\n",
    "VAL_DEPTH_DIR = \"Datasets/Validation/depths/\"\n",
    "TEST_IMG_DIR = \"Datasets/Test/images/\"\n",
    "TEST_DEPTH_DIR = \"Datasets/Test/depths/\"\n",
    "\n",
    "IMAGE_HEIGHT = 120\n",
    "IMAGE_WIDTH = 160\n",
    "\n",
    "MODEL_NAME = 'New_Model'\n",
    "MODEL_SAVE_DIR = \"Models/New_Model/checkpoint/\"\n",
    "MODEL_LOAD_PATH = \"Models/New_Model/checkpoint/\" + MODEL_NAME + \"_80.pth.tar\"\n",
    "VALIDATION_IMAGES_SAVE_DIR = \"Models/New_Model/validation_outputs/\"\n",
    "\n",
    "TRAIN_SAVE_PATH = \"Models/New_Model/predictions/Train/\"\n",
    "\n",
    "dtype=torch.cuda.FloatTensor\n",
    "\n",
    "def train_unet(loader, model, optimizer, loss_fn, scaler):\n",
    "    # loop = tqdm(loader)\n",
    "\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, gradient, targets) in enumerate(loader):\n",
    "        data = data.to(device=DEVICE)\n",
    "        gradient = gradient.to(device=DEVICE)\n",
    "        targets = targets.to(device=DEVICE)\n",
    "\n",
    "        # forward\n",
    "        predictions = model(data.type(dtype), gradient.type(dtype))\n",
    "        loss = loss_fn(predictions, targets)\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # scaler.scale(loss).backward()\n",
    "        # scaler.step(optimizer)\n",
    "        # scaler.update()\n",
    "\n",
    "        # update tqdm loop\n",
    "        # loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    train_loss /= (batch_idx + 1)\n",
    "    return train_loss\n",
    "\n",
    "def validate_unet(loader, model, loss_fn, epoch, train_loss, save_folder):\n",
    "  # loop = tqdm(loader)\n",
    "\n",
    "  validation_loss = 0\n",
    "  scale_invariant_loss = 0\n",
    "  delta1_accuracy = 0\n",
    "  delta2_accuracy = 0\n",
    "  delta3_accuracy = 0\n",
    "  rmse_linear_loss = 0\n",
    "  rmse_log_loss = 0\n",
    "  abs_relative_difference_loss = 0\n",
    "  squared_relative_difference_loss = 0\n",
    "\n",
    "  model.eval()\n",
    "  for batch_idx, (data, gradient, targets) in enumerate(loader):\n",
    "      data = data.to(device=DEVICE)\n",
    "      gradient = gradient.to(device=DEVICE)\n",
    "      targets = targets.to(device=DEVICE)\n",
    "\n",
    "      with torch.no_grad():\n",
    "        predictions = model(data.type(dtype), gradient.type(dtype))\n",
    "        loss = loss_fn(predictions, targets)\n",
    "      \n",
    "      validation_loss += loss.item()\n",
    "\n",
    "      # Error function\n",
    "      scale_invariant_loss += loss_fn(predictions, targets)\n",
    "      delta1_accuracy += threeshold_percentage(predictions, targets, 1.25)\n",
    "      delta2_accuracy += threeshold_percentage(predictions, targets, 1.25*1.25)\n",
    "      delta3_accuracy += threeshold_percentage(predictions, targets, 1.25*1.25*1.25)\n",
    "      rmse_linear_loss += rmse_linear(predictions, targets)\n",
    "      rmse_log_loss += rmse_log(predictions, targets)\n",
    "      abs_relative_difference_loss += abs_relative_difference(predictions, targets)\n",
    "      squared_relative_difference_loss += squared_relative_difference(predictions, targets)\n",
    "\n",
    "      # Saving output depths\n",
    "      targets -= torch.min(targets)\n",
    "      targets = targets/torch.max(targets)\n",
    "\n",
    "      predictions -= torch.min(predictions)\n",
    "      predictions = predictions/torch.max(predictions)\n",
    "\n",
    "      torchvision.utils.save_image(predictions, f\"{save_folder}/pred_{batch_idx}.png\")\n",
    "      torchvision.utils.save_image(targets, f\"{save_folder}{batch_idx}.png\")\n",
    "      \n",
    "      # update tqdm loop\n",
    "      # loop.set_postfix(validation_loss=loss.item())\n",
    "  \n",
    "  validation_loss /= (batch_idx + 1)\n",
    "  delta1_accuracy /= (batch_idx + 1)\n",
    "  delta2_accuracy /= (batch_idx + 1)\n",
    "  delta3_accuracy /= (batch_idx + 1)\n",
    "  rmse_linear_loss /= (batch_idx + 1)\n",
    "  rmse_log_loss /= (batch_idx + 1)\n",
    "  abs_relative_difference_loss /= (batch_idx + 1)\n",
    "  squared_relative_difference_loss /= (batch_idx + 1)\n",
    "\n",
    "  print('Epoch: {}    {:.4f}      {:.4f}      {:.4f}      {:.4f}      {:.4f}      {:.4f}      {:.4f}      {:.4f}      {:.4f}'.format(epoch, train_loss, \n",
    "        validation_loss, delta1_accuracy, delta2_accuracy, delta3_accuracy, rmse_linear_loss, rmse_log_loss, \n",
    "        abs_relative_difference_loss, squared_relative_difference_loss))\n",
    "  \n",
    "  model.train()\n",
    "  return validation_loss\n",
    "\n",
    "def main():\n",
    "    rgb_data_transforms = transforms.Compose([\n",
    "        transforms.Resize((IMAGE_HEIGHT, IMAGE_WIDTH)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    depth_data_transforms = transforms.Compose([\n",
    "        TransposeDepthInput(),\n",
    "    ])\n",
    "\n",
    "    train_loader, val_loader, test_loader = get_loaders(\n",
    "          TRAIN_IMG_DIR,\n",
    "          TRAIN_DEPTH_DIR,\n",
    "          VAL_IMG_DIR,\n",
    "          VAL_DEPTH_DIR,\n",
    "          TEST_IMG_DIR,\n",
    "          TEST_DEPTH_DIR,\n",
    "          BATCH_SIZE,\n",
    "          rgb_data_transforms,\n",
    "          depth_data_transforms,\n",
    "          NUM_WORKERS,\n",
    "          PIN_MEMORY,\n",
    "    )\n",
    "\n",
    "    model = YNET(in_channels1=3, in_channels2=2, out_channels=1).to(DEVICE)\n",
    "    loss_fn = ScaleInvariantLoss\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    # scaler = torch.cuda.amp.GradScaler()\n",
    "    scaler = None\n",
    "    \n",
    "    train_losses, validation_losses = [], []\n",
    "\n",
    "    if LOAD_MODEL:\n",
    "      print(\"=> Loading Chekpoint\")\n",
    "      checkpoint = torch.load(MODEL_LOAD_PATH)\n",
    "      model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "      train_losses = checkpoint[\"train_losses\"]\n",
    "      validation_losses = checkpoint[\"validation_losses\"]\n",
    "      print(\"=> Checkpoint Loaded\")\n",
    "\n",
    "    print(\"********* Training the New Model **************\")\n",
    "    print(\"Epochs:     Train_loss  Val_loss    Delta_1     Delta_2     Delta_3    rmse_lin    rmse_log    abs_rel.  square_relative\")\n",
    "    print(\"Paper Val:                          (0.618)     (0.891)     (0.969)     (0.871)     (0.283)     (0.228)     (0.223)\")\n",
    "    \n",
    "    file_names = ['31.png', '69.png', '1020.png', '1021.png']\n",
    "\n",
    "    for epoch in range(81, NUM_EPOCHS+1):\n",
    "        train_loss = train_unet(train_loader, model, optimizer, loss_fn, scaler)\n",
    "        validation_loss = validate_unet(val_loader, model, loss_fn, epoch, train_loss, save_folder=VALIDATION_IMAGES_SAVE_DIR)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        validation_losses.append(validation_loss)\n",
    "        \n",
    "        Save_Predictions(model, file_names, TRAIN_IMG_DIR, TRAIN_DEPTH_DIR, TRAIN_SAVE_PATH)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "          # save model\n",
    "          checkpoint = {\n",
    "              \"state_dict\": model.state_dict(),\n",
    "              \"train_losses\": train_losses,\n",
    "              \"validation_losses\": validation_losses,\n",
    "          }\n",
    "          save_path = MODEL_SAVE_DIR + MODEL_NAME + '_' + str(epoch) + '.pth.tar'\n",
    "          save_checkpoint(checkpoint, save_path)\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lpxupkCroTDK",
    "outputId": "fb910590-ca96-47b2-9081-f8cae8bf876c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********* Training the New Model **************\n",
      "Epochs:     Train_loss  Val_loss    Delta_1     Delta_2     Delta_3    rmse_lin    rmse_log    abs_rel.  square_relative\n",
      "Paper Val:                          (0.618)     (0.891)     (0.969)     (0.871)     (0.283)     (0.228)     (0.223)\n",
      "Epoch: 1    0.3090      0.2550      0.1471      0.3852      0.6712      1.3462      0.3974      0.4675      0.7237\n",
      "Epoch: 2    0.2140      0.1999      0.2205      0.5164      0.7900      1.1949      0.2937      0.4236      0.6265\n",
      "Epoch: 3    0.1920      0.1247      0.4514      0.7878      0.9449      0.8878      0.1513      0.3481      0.5098\n",
      "Epoch: 4    0.1770      0.1363      0.4037      0.7335      0.9232      0.9622      0.1735      0.3522      0.5030\n",
      "Epoch: 5    0.1686      0.1023      0.5908      0.8755      0.9672      0.7389      0.1148      0.3278      0.5131\n",
      "Epoch: 6    0.1586      0.1510      0.3520      0.6815      0.8916      1.0154      0.2056      0.3579      0.5005\n",
      "Epoch: 7    0.1542      0.1474      0.3524      0.6899      0.8937      1.0031      0.2009      0.3664      0.5414\n",
      "Epoch: 8    0.1458      0.1010      0.5943      0.8813      0.9680      0.7332      0.1135      0.3254      0.5126\n",
      "Epoch: 9    0.1404      0.1060      0.5751      0.8638      0.9622      0.7360      0.1201      0.3198      0.5006\n",
      "Epoch: 10    0.1356      0.1026      0.5938      0.8792      0.9671      0.7390      0.1153      0.3321      0.5424\n",
      "=> Saving checkpoint\n",
      "Epoch: 11    0.1337      0.1034      0.5962      0.8771      0.9612      0.7368      0.1180      0.3509      0.5656\n",
      "Epoch: 12    0.1270      0.1059      0.5712      0.8636      0.9600      0.7546      0.1214      0.3187      0.4941\n",
      "Epoch: 13    0.1206      0.1081      0.5478      0.8495      0.9600      0.7659      0.1267      0.3256      0.5302\n",
      "Epoch: 14    0.1189      0.1047      0.5978      0.8783      0.9608      0.7272      0.1166      0.3240      0.5247\n",
      "Epoch: 15    0.1110      0.1157      0.5384      0.8387      0.9518      0.8054      0.1351      0.3382      0.5769\n",
      "Epoch: 16    0.1071      0.1052      0.5995      0.8727      0.9594      0.7316      0.1180      0.3202      0.5060\n",
      "Epoch: 17    0.1035      0.1002      0.6107      0.8892      0.9667      0.7113      0.1114      0.3261      0.5420\n",
      "Epoch: 18    0.1018      0.1159      0.5491      0.8446      0.9505      0.8046      0.1348      0.3208      0.4875\n",
      "Epoch: 19    0.0974      0.1171      0.5278      0.8325      0.9515      0.7790      0.1373      0.3349      0.5621\n",
      "Epoch: 20    0.0915      0.1188      0.5374      0.8305      0.9427      0.7800      0.1400      0.3176      0.4800\n",
      "=> Saving checkpoint\n",
      "Epoch: 21    0.0878      0.1016      0.6051      0.8763      0.9641      0.7134      0.1143      0.3172      0.5184\n",
      "Epoch: 22    0.0840      0.1081      0.5914      0.8702      0.9619      0.7215      0.1220      0.3320      0.5915\n",
      "Epoch: 23    0.0802      0.1058      0.6066      0.8790      0.9613      0.7155      0.1185      0.3147      0.5224\n",
      "Epoch: 24    0.0775      0.1091      0.5946      0.8769      0.9607      0.7418      0.1221      0.3300      0.5878\n",
      "Epoch: 25    0.0777      0.1041      0.6188      0.8800      0.9610      0.7021      0.1159      0.3259      0.5599\n",
      "Epoch: 26    0.0768      0.1136      0.5236      0.8364      0.9499      0.7958      0.1351      0.3214      0.5038\n",
      "Epoch: 27    0.0725      0.1097      0.6165      0.8839      0.9652      0.7435      0.1208      0.3391      0.6195\n",
      "Epoch: 28    0.0709      0.1003      0.5954      0.8839      0.9689      0.7151      0.1120      0.3175      0.5390\n",
      "Epoch: 29    0.0665      0.0971      0.6253      0.8924      0.9697      0.6921      0.1076      0.3148      0.5229\n",
      "Epoch: 30    0.0647      0.1070      0.5870      0.8712      0.9622      0.7412      0.1204      0.3208      0.5511\n",
      "=> Saving checkpoint\n",
      "Epoch: 31    0.0625      0.1149      0.5820      0.8538      0.9550      0.7312      0.1296      0.3332      0.6059\n",
      "Epoch: 32    0.0616      0.1026      0.6234      0.8843      0.9658      0.7005      0.1148      0.3092      0.5182\n",
      "Epoch: 33    0.0593      0.1010      0.6250      0.8998      0.9694      0.7155      0.1116      0.3374      0.6363\n",
      "Epoch: 34    0.0577      0.1034      0.5892      0.8779      0.9683      0.7180      0.1166      0.3265      0.5724\n",
      "Epoch: 35    0.0581      0.0977      0.6194      0.8918      0.9691      0.6972      0.1097      0.3032      0.4942\n",
      "Epoch: 36    0.0544      0.0988      0.6414      0.8990      0.9691      0.6991      0.1088      0.3181      0.5495\n",
      "Epoch: 37    0.0529      0.1046      0.5709      0.8631      0.9601      0.7461      0.1216      0.3041      0.4648\n",
      "Epoch: 38    0.0515      0.1066      0.6064      0.8896      0.9689      0.7543      0.1178      0.3630      0.7665\n",
      "Epoch: 39    0.0496      0.0996      0.6159      0.8964      0.9694      0.7218      0.1099      0.3299      0.5894\n",
      "Epoch: 40    0.0470      0.0952      0.6439      0.8987      0.9702      0.6831      0.1050      0.3142      0.5344\n",
      "=> Saving checkpoint\n",
      "Epoch: 41    0.0451      0.0958      0.6533      0.9013      0.9680      0.6838      0.1052      0.3092      0.5088\n",
      "Epoch: 42    0.0443      0.0985      0.6119      0.8858      0.9703      0.7060      0.1102      0.3083      0.5197\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "rhjAKIoDwKxo",
    "outputId": "0d694222-e3d2-4933-e755-b8eede30bc03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Loading Chekpoint\n",
      "=> Checkpoint Loaded\n",
      "********* Training the New Model **************\n",
      "Epochs:     Train_loss  Val_loss    Delta_1     Delta_2     Delta_3    rmse_lin    rmse_log    abs_rel.  square_relative\n",
      "Paper Val:                          (0.618)     (0.891)     (0.969)     (0.871)     (0.283)     (0.228)     (0.223)\n",
      "Epoch: 41    0.0483      0.1004      0.6336      0.8887      0.9643      0.7086      0.1109      0.3252      0.5460\n",
      "Epoch: 42    0.0467      0.1004      0.6387      0.8946      0.9653      0.6932      0.1107      0.3077      0.5111\n",
      "Epoch: 43    0.0468      0.1015      0.6019      0.8778      0.9663      0.7251      0.1148      0.3160      0.5674\n",
      "Epoch: 44    0.0436      0.0923      0.6378      0.9035      0.9732      0.6765      0.1026      0.3005      0.4839\n",
      "Epoch: 45    0.0421      0.0979      0.6518      0.9008      0.9689      0.6851      0.1071      0.3105      0.5422\n",
      "Epoch: 46    0.0417      0.0983      0.6240      0.8891      0.9689      0.6981      0.1099      0.3125      0.5327\n",
      "Epoch: 47    0.0412      0.0968      0.6287      0.8973      0.9721      0.6897      0.1074      0.3182      0.5876\n",
      "Epoch: 48    0.0392      0.0962      0.6411      0.8969      0.9700      0.6832      0.1063      0.3094      0.5478\n",
      "Epoch: 49    0.0381      0.0928      0.6532      0.9091      0.9753      0.6619      0.1023      0.3099      0.5555\n",
      "Epoch: 50    0.0367      0.0956      0.6343      0.8998      0.9724      0.6838      0.1063      0.3148      0.5612\n",
      "=> Saving checkpoint\n",
      "Epoch: 51    0.0350      0.0963      0.6084      0.8852      0.9695      0.7043      0.1093      0.3009      0.4736\n",
      "Epoch: 52    0.0351      0.0941      0.6552      0.9074      0.9739      0.6700      0.1035      0.3147      0.5667\n",
      "Epoch: 53    0.0335      0.0949      0.6362      0.8986      0.9722      0.6756      0.1055      0.3074      0.5310\n",
      "Epoch: 54    0.0321      0.0930      0.6526      0.9015      0.9702      0.6801      0.1024      0.3040      0.4940\n",
      "Epoch: 55    0.0315      0.0941      0.6463      0.9008      0.9718      0.6713      0.1044      0.3083      0.5325\n",
      "Epoch: 56    0.0310      0.0965      0.6147      0.8900      0.9689      0.6940      0.1090      0.3041      0.4993\n",
      "Epoch: 57    0.0316      0.0947      0.6346      0.8980      0.9729      0.6781      0.1055      0.3073      0.5290\n",
      "Epoch: 58    0.0299      0.0918      0.6572      0.9067      0.9724      0.6609      0.1014      0.3034      0.5127\n",
      "Epoch: 59    0.0299      0.0943      0.6514      0.8985      0.9702      0.6798      0.1037      0.3031      0.4833\n",
      "Epoch: 60    0.0304      0.0909      0.6658      0.9114      0.9727      0.6564      0.0999      0.3042      0.5187\n",
      "=> Saving checkpoint\n",
      "Epoch: 61    0.0296      0.0956      0.6403      0.9077      0.9754      0.6707      0.1053      0.3173      0.6090\n",
      "Epoch: 62    0.0297      0.0951      0.6574      0.9059      0.9724      0.6709      0.1042      0.3144      0.5763\n",
      "Epoch: 63    0.0274      0.0947      0.6305      0.8969      0.9723      0.6813      0.1059      0.3071      0.5165\n",
      "Epoch: 64    0.0272      0.0926      0.6618      0.9068      0.9726      0.6637      0.1020      0.3125      0.5569\n",
      "Epoch: 65    0.0276      0.0936      0.6526      0.9065      0.9735      0.6635      0.1035      0.3122      0.5633\n",
      "Epoch: 66    0.0266      0.0914      0.6362      0.9041      0.9736      0.6722      0.1026      0.3000      0.4859\n",
      "Epoch: 67    0.0268      0.0952      0.6061      0.8861      0.9711      0.7104      0.1087      0.3010      0.4790\n",
      "Epoch: 68    0.0265      0.0902      0.6729      0.9124      0.9748      0.6473      0.0992      0.3129      0.5537\n",
      "Epoch: 69    0.0262      0.0919      0.6584      0.9094      0.9747      0.6575      0.1012      0.3086      0.5525\n",
      "Epoch: 70    0.0262      0.0956      0.6231      0.8949      0.9713      0.6812      0.1077      0.3064      0.5187\n",
      "=> Saving checkpoint\n",
      "Epoch: 71    0.0262      0.1003      0.6211      0.8907      0.9686      0.6942      0.1119      0.3136      0.5694\n",
      "Epoch: 72    0.0264      0.0962      0.6310      0.8929      0.9725      0.6889      0.1075      0.3084      0.5392\n",
      "Epoch: 73    0.0255      0.0901      0.6717      0.9158      0.9774      0.6438      0.0996      0.3207      0.5991\n",
      "Epoch: 74    0.0225      0.0892      0.6634      0.9148      0.9782      0.6479      0.0983      0.3082      0.5589\n",
      "Epoch: 75    0.0215      0.0951      0.6459      0.9083      0.9755      0.6694      0.1042      0.3173      0.6092\n",
      "Epoch: 76    0.0214      0.0920      0.6346      0.8979      0.9725      0.6797      0.1037      0.2971      0.4806\n",
      "Epoch: 77    0.0223      0.0898      0.6427      0.9066      0.9766      0.6651      0.1005      0.3006      0.4932\n",
      "Epoch: 78    0.0223      0.0919      0.6486      0.9071      0.9755      0.6626      0.1018      0.3080      0.5471\n",
      "Epoch: 79    0.0218      0.0895      0.6747      0.9168      0.9766      0.6466      0.0985      0.3169      0.5948\n",
      "Epoch: 80    0.0218      0.0947      0.6354      0.8996      0.9742      0.6799      0.1058      0.3100      0.5481\n",
      "=> Saving checkpoint\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-b19e977969f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-648893f0f5f7>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m41\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_unet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m         \u001b[0mvalidation_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_unet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_folder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mVALIDATION_IMAGES_SAVE_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-648893f0f5f7>\u001b[0m in \u001b[0;36mtrain_unet\u001b[0;34m(loader, model, optimizer, loss_fn, scaler)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;31m# backward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tlgm7T_jwK5U",
    "outputId": "3194d7c7-51d3-45ca-ef3c-c7a663e3439b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Loading Chekpoint\n",
      "=> Checkpoint Loaded\n",
      "********* Training the New Model **************\n",
      "Epochs:     Train_loss  Val_loss    Delta_1     Delta_2     Delta_3    rmse_lin    rmse_log    abs_rel.  square_relative\n",
      "Paper Val:                          (0.618)     (0.891)     (0.969)     (0.871)     (0.283)     (0.228)     (0.223)\n",
      "Epoch: 81    0.0229      0.0913      0.6617      0.9096      0.9746      0.6610      0.1005      0.3112      0.5548\n",
      "Epoch: 82    0.0225      0.0914      0.6489      0.9074      0.9755      0.6612      0.1016      0.3026      0.5225\n",
      "Epoch: 83    0.0229      0.0910      0.6582      0.9104      0.9751      0.6597      0.1006      0.3064      0.5403\n",
      "Epoch: 84    0.0217      0.0929      0.6670      0.9038      0.9723      0.6644      0.1021      0.3129      0.5405\n",
      "Epoch: 85    0.0220      0.0906      0.6530      0.9118      0.9759      0.6566      0.1006      0.3048      0.5216\n",
      "Epoch: 86    0.0215      0.0942      0.6600      0.9041      0.9727      0.6726      0.1039      0.3085      0.5434\n",
      "Epoch: 87    0.0212      0.0907      0.6634      0.9104      0.9744      0.6539      0.1003      0.3066      0.5449\n",
      "Epoch: 88    0.0209      0.0896      0.6745      0.9146      0.9756      0.6492      0.0985      0.3046      0.5346\n",
      "Epoch: 89    0.0197      0.0906      0.6483      0.9045      0.9754      0.6622      0.1010      0.3004      0.5128\n",
      "Epoch: 90    0.0197      0.0918      0.6681      0.9138      0.9751      0.6523      0.1009      0.3115      0.5769\n",
      "=> Saving checkpoint\n",
      "Epoch: 91    0.0192      0.0920      0.6610      0.9096      0.9743      0.6576      0.1014      0.3107      0.5613\n",
      "Epoch: 92    0.0189      0.0915      0.6419      0.9028      0.9746      0.6723      0.1024      0.2988      0.4931\n",
      "Epoch: 93    0.0195      0.0983      0.6304      0.8972      0.9713      0.6848      0.1089      0.3159      0.5890\n",
      "Epoch: 94    0.0193      0.0934      0.6451      0.9042      0.9739      0.6713      0.1038      0.3070      0.5490\n",
      "Epoch: 95    0.0194      0.0933      0.6175      0.8954      0.9730      0.6945      0.1059      0.3011      0.4818\n",
      "Epoch: 96    0.0168      0.0901      0.6718      0.9122      0.9764      0.6483      0.0993      0.3083      0.5569\n",
      "Epoch: 97    0.0173      0.0913      0.6594      0.9056      0.9742      0.6635      0.1009      0.3046      0.5173\n",
      "Epoch: 98    0.0167      0.0882      0.6526      0.9119      0.9772      0.6537      0.0985      0.2971      0.4910\n",
      "Epoch: 99    0.0171      0.0925      0.6660      0.9134      0.9750      0.6608      0.1014      0.3180      0.6018\n",
      "Epoch: 100    0.0174      0.0931      0.6377      0.9010      0.9741      0.6740      0.1039      0.3039      0.5192\n",
      "=> Saving checkpoint\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y5a0xXj4a474"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S0NTtKVj72NQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RnX-LhO472Pi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ICzn4lbDxdrk"
   },
   "source": [
    "### **Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XfE4MfUbxdKK",
    "outputId": "dd4d7170-49c3-4d50-dc4f-a1927de262f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/Neural Network Project\n"
     ]
    }
   ],
   "source": [
    "%cd /content/drive/MyDrive/Neural\\ Network\\ Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uhLkY0c1xdKP"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "from Ynet import YNET\n",
    "from DataLoader import TransposeDepthInput, NYUDataset, save_checkpoint, get_loaders, save_predictions_as_imgs\n",
    "from metrics import ScaleInvariantLoss, threeshold_percentage, rmse_linear, rmse_log, abs_relative_difference, squared_relative_difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mp1PIEApxhHQ"
   },
   "outputs": [],
   "source": [
    "IMAGE_HEIGHT = 120\n",
    "IMAGE_WIDTH = 160\n",
    "\n",
    "rgb_data_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_HEIGHT, IMAGE_WIDTH)),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pzQsrws-xhPO"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "YNET_MODEL_PATH = \"Models/Ynet/checkpoint/Ynet_model_100.pth.tar\"\n",
    "TRAIN_SAVE_PATH = \"Models/Ynet/predictions/Train/\"\n",
    "VAL_SAVE_PATH = \"Models/Ynet/predictions/Validation/\"\n",
    "TEST_SAVE_PATH = \"Models/Ynet/predictions/Test/\"\n",
    "\n",
    "TRAIN_IMG_DIR = \"Datasets/Train/images/\"\n",
    "TRAIN_DEPTH_DIR = \"Datasets/Train/depths/\"\n",
    "VAL_IMG_DIR = \"Datasets/Validation/images/\"\n",
    "VAL_DEPTH_DIR = \"Datasets/Validation/depths/\"\n",
    "TEST_IMG_DIR = \"Datasets/Test/images/\"\n",
    "TEST_DEPTH_DIR = \"Datasets/Test/depths/\"\n",
    "\n",
    "model = YNET(in_channels1=3, in_channels2=2, out_channels=1).to(DEVICE)\n",
    "\n",
    "# Loading Unet model\n",
    "checkpoint = torch.load(YNET_MODEL_PATH)\n",
    "model.load_state_dict(checkpoint[\"state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_hv_32ZF143-"
   },
   "outputs": [],
   "source": [
    "def Save_Predictions(image_dir, depth_dir, save_dir):\n",
    "    model.eval()\n",
    "    for image_name in os.listdir(image_dir):\n",
    "        # Load the image and dpeth\n",
    "        image = cv2.imread(image_dir + image_name, cv2.IMREAD_UNCHANGED)\n",
    "        depth = cv2.imread(depth_dir+ image_name, cv2.IMREAD_UNCHANGED)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = Image.fromarray(image)\n",
    "        image = rgb_data_transforms(image)\n",
    "\n",
    "        # Find the gradient\n",
    "        gray = np.moveaxis(image.numpy(), [0, 1, 2], [2, 0, 1])\n",
    "        gray = cv2.cvtColor(gray, cv2.COLOR_BGR2GRAY)\n",
    "        gx = cv2.Sobel(gray, ddepth = cv2.CV_32F, dx=1, dy=0, ksize=3)\n",
    "        gy = cv2.Sobel(gray, ddepth = cv2.CV_32F, dx=0, dy=1, ksize=3)\n",
    "        gradient = torch.from_numpy(np.stack([gx, gy]))\n",
    "\n",
    "        image = torch.unsqueeze(image, 0)\n",
    "        gradient = torch.unsqueeze(gradient, 0)\n",
    "\n",
    "        # Predict the output\n",
    "        image = image.to(device=DEVICE)\n",
    "        gradient = gradient.to(device=DEVICE)\n",
    "        with torch.no_grad():\n",
    "            predicted = model(image, gradient)\n",
    "\n",
    "        image = image.cpu()\n",
    "        predicted = predicted.cpu()\n",
    "\n",
    "        input_image = np.zeros((120, 160, 3), dtype=np.float32)\n",
    "        input_image[:, :, 0] = image[0, 0, :, :]\n",
    "        input_image[:, :, 1] = image[0, 1, :, :]\n",
    "        input_image[:, :, 2] = image[0, 2, :, :]\n",
    "        predicted = predicted[0, 0, :, :]\n",
    "\n",
    "        fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "        ax = fig.add_subplot(1, 3, 1)\n",
    "        ax.set_title('Input image')\n",
    "        plt.imshow(input_image)\n",
    "        ax = fig.add_subplot(1, 3, 2)\n",
    "        ax.set_title('Ground truth')\n",
    "        plt.imshow(depth, cmap='gist_gray')    #plt.imshow(actual_depth, cmap='jet')\n",
    "        ax = fig.add_subplot(1, 3, 3)\n",
    "        ax.set_title('Ynet predicted')\n",
    "        plt.imshow(predicted, cmap='gist_gray')\n",
    "        plt.savefig(f'{save_dir}/{image_name}')\n",
    "        plt.close(fig)\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n2hLrZ2j1xEi"
   },
   "outputs": [],
   "source": [
    "Save_Predictions(VAL_IMG_DIR, VAL_DEPTH_DIR, VAL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vBlw9zbz-Jlb"
   },
   "outputs": [],
   "source": [
    "Save_Predictions(TRAIN_IMG_DIR, TRAIN_DEPTH_DIR, TRAIN_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I3e4lIbu34u9"
   },
   "outputs": [],
   "source": [
    "Save_Predictions(TEST_IMG_DIR, TEST_DEPTH_DIR, TEST_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TFk9KCDK8H44"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q1lP_oP4QJ_u"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y8aUbM_6OUZh"
   },
   "source": [
    "### **Time taken**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YUCw66JVOT6q",
    "outputId": "de3b29bd-ec4a-4565-acdb-5c2794384364"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.11543568611145019\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "model.eval()\n",
    "start_time = time.time()\n",
    "num_images = 50\n",
    "for i in range(num_images):\n",
    "    image = cv2.imread(TRAIN_IMG_DIR + str(i) + '.png', cv2.IMREAD_UNCHANGED)\n",
    "    depth = cv2.imread(TRAIN_DEPTH_DIR + str(i) + '.png', cv2.IMREAD_UNCHANGED)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = Image.fromarray(image)\n",
    "    image = rgb_data_transforms(image)\n",
    "\n",
    "    # Find the gradient\n",
    "    gray = np.moveaxis(image.numpy(), [0, 1, 2], [2, 0, 1])\n",
    "    gray = cv2.cvtColor(gray, cv2.COLOR_BGR2GRAY)\n",
    "    gx = cv2.Sobel(gray, ddepth = cv2.CV_32F, dx=1, dy=0, ksize=3)\n",
    "    gy = cv2.Sobel(gray, ddepth = cv2.CV_32F, dx=0, dy=1, ksize=3)\n",
    "    gradient = torch.from_numpy(np.stack([gx, gy]))\n",
    "\n",
    "    image = torch.unsqueeze(image, 0)\n",
    "    gradient = torch.unsqueeze(gradient, 0)\n",
    "\n",
    "    # Predict the output\n",
    "    image = image.to(device=DEVICE)\n",
    "    gradient = gradient.to(device=DEVICE)\n",
    "    with torch.no_grad():\n",
    "        predicted = model(image, gradient)\n",
    "end_time = time.time()\n",
    "model.train()\n",
    "print('Time taken:', (end_time-start_time)/num_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T7V-jG1QQw9r"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JGN0O-RLQxAB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gx7Ry4e2Qxie"
   },
   "source": [
    "### **Model summary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cpNZk-K8SGus",
    "outputId": "2b89aa4f-5a64-4a57-b95d-8ebd661b5f9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YNET(\n",
      "  (ups): ModuleList()\n",
      "  (downConvs1): DownConv(\n",
      "    (downs): ModuleList(\n",
      "      (0): DoubleConv(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=same, bias=False)\n",
      "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same, bias=False)\n",
      "          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (5): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (1): DoubleConv(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=same, bias=False)\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same, bias=False)\n",
      "          (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (5): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (2): DoubleConv(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=same, bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same, bias=False)\n",
      "          (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (5): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (3): DoubleConv(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=same, bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=same, bias=False)\n",
      "          (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (5): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (residualBlocks): ModuleList(\n",
      "      (0): DoubleConv(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=same, dilation=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=same, dilation=(2, 2), bias=False)\n",
      "          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (5): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (1): DoubleConv(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2d(128, 128, kernel_size=(5, 5), stride=(1, 1), padding=same, dilation=(4, 4), bias=False)\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): Conv2d(128, 128, kernel_size=(5, 5), stride=(1, 1), padding=same, dilation=(4, 4), bias=False)\n",
      "          (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (5): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (2): DoubleConv(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=same, dilation=(8, 8), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=same, dilation=(8, 8), bias=False)\n",
      "          (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (5): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (3): DoubleConv(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2d(512, 512, kernel_size=(5, 5), stride=(1, 1), padding=same, dilation=(16, 16), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): Conv2d(512, 512, kernel_size=(5, 5), stride=(1, 1), padding=same, dilation=(16, 16), bias=False)\n",
      "          (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (5): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (downConvs2): DownConv(\n",
      "    (downs): ModuleList(\n",
      "      (0): DoubleConv(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=same, bias=False)\n",
      "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same, bias=False)\n",
      "          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (5): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (1): DoubleConv(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=same, bias=False)\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same, bias=False)\n",
      "          (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (5): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (2): DoubleConv(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=same, bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same, bias=False)\n",
      "          (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (5): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (3): DoubleConv(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=same, bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=same, bias=False)\n",
      "          (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (5): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (residualBlocks): ModuleList(\n",
      "      (0): DoubleConv(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=same, dilation=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=same, dilation=(2, 2), bias=False)\n",
      "          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (5): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (1): DoubleConv(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2d(128, 128, kernel_size=(5, 5), stride=(1, 1), padding=same, dilation=(4, 4), bias=False)\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): Conv2d(128, 128, kernel_size=(5, 5), stride=(1, 1), padding=same, dilation=(4, 4), bias=False)\n",
      "          (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (5): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (2): DoubleConv(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=same, dilation=(8, 8), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=same, dilation=(8, 8), bias=False)\n",
      "          (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (5): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (3): DoubleConv(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2d(512, 512, kernel_size=(5, 5), stride=(1, 1), padding=same, dilation=(16, 16), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): Conv2d(512, 512, kernel_size=(5, 5), stride=(1, 1), padding=same, dilation=(16, 16), bias=False)\n",
      "          (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (5): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (bottleneck): DoubleConv(\n",
      "    (conv): Sequential(\n",
      "      (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=same, bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=same, bias=False)\n",
      "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (upConvs): UpConv(\n",
      "    (ups): ModuleList(\n",
      "      (0): ConvTranspose2d(512, 512, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (1): DoubleConv(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2d(1536, 256, kernel_size=(3, 3), stride=(1, 1), padding=same, bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same, bias=False)\n",
      "          (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (5): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (2): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (3): DoubleConv(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2d(768, 128, kernel_size=(3, 3), stride=(1, 1), padding=same, bias=False)\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same, bias=False)\n",
      "          (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (5): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (4): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (5): DoubleConv(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2d(384, 64, kernel_size=(3, 3), stride=(1, 1), padding=same, bias=False)\n",
      "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same, bias=False)\n",
      "          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (5): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (6): ConvTranspose2d(64, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (7): DoubleConv(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2d(192, 32, kernel_size=(3, 3), stride=(1, 1), padding=same, bias=False)\n",
      "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=same, bias=False)\n",
      "          (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (5): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (downConvs1): DownConv(\n",
      "      (downs): ModuleList(\n",
      "        (0): DoubleConv(\n",
      "          (conv): Sequential(\n",
      "            (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=same, bias=False)\n",
      "            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU(inplace=True)\n",
      "            (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same, bias=False)\n",
      "            (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (5): ReLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (1): DoubleConv(\n",
      "          (conv): Sequential(\n",
      "            (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=same, bias=False)\n",
      "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU(inplace=True)\n",
      "            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same, bias=False)\n",
      "            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (5): ReLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (2): DoubleConv(\n",
      "          (conv): Sequential(\n",
      "            (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=same, bias=False)\n",
      "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU(inplace=True)\n",
      "            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same, bias=False)\n",
      "            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (5): ReLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (3): DoubleConv(\n",
      "          (conv): Sequential(\n",
      "            (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=same, bias=False)\n",
      "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU(inplace=True)\n",
      "            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=same, bias=False)\n",
      "            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (5): ReLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (residualBlocks): ModuleList(\n",
      "        (0): DoubleConv(\n",
      "          (conv): Sequential(\n",
      "            (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=same, dilation=(2, 2), bias=False)\n",
      "            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU(inplace=True)\n",
      "            (3): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=same, dilation=(2, 2), bias=False)\n",
      "            (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (5): ReLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (1): DoubleConv(\n",
      "          (conv): Sequential(\n",
      "            (0): Conv2d(128, 128, kernel_size=(5, 5), stride=(1, 1), padding=same, dilation=(4, 4), bias=False)\n",
      "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU(inplace=True)\n",
      "            (3): Conv2d(128, 128, kernel_size=(5, 5), stride=(1, 1), padding=same, dilation=(4, 4), bias=False)\n",
      "            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (5): ReLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (2): DoubleConv(\n",
      "          (conv): Sequential(\n",
      "            (0): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=same, dilation=(8, 8), bias=False)\n",
      "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU(inplace=True)\n",
      "            (3): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=same, dilation=(8, 8), bias=False)\n",
      "            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (5): ReLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (3): DoubleConv(\n",
      "          (conv): Sequential(\n",
      "            (0): Conv2d(512, 512, kernel_size=(5, 5), stride=(1, 1), padding=same, dilation=(16, 16), bias=False)\n",
      "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU(inplace=True)\n",
      "            (3): Conv2d(512, 512, kernel_size=(5, 5), stride=(1, 1), padding=same, dilation=(16, 16), bias=False)\n",
      "            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (5): ReLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (downConvs2): DownConv(\n",
      "      (downs): ModuleList(\n",
      "        (0): DoubleConv(\n",
      "          (conv): Sequential(\n",
      "            (0): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=same, bias=False)\n",
      "            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU(inplace=True)\n",
      "            (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same, bias=False)\n",
      "            (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (5): ReLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (1): DoubleConv(\n",
      "          (conv): Sequential(\n",
      "            (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=same, bias=False)\n",
      "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU(inplace=True)\n",
      "            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same, bias=False)\n",
      "            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (5): ReLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (2): DoubleConv(\n",
      "          (conv): Sequential(\n",
      "            (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=same, bias=False)\n",
      "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU(inplace=True)\n",
      "            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same, bias=False)\n",
      "            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (5): ReLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (3): DoubleConv(\n",
      "          (conv): Sequential(\n",
      "            (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=same, bias=False)\n",
      "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU(inplace=True)\n",
      "            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=same, bias=False)\n",
      "            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (5): ReLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (residualBlocks): ModuleList(\n",
      "        (0): DoubleConv(\n",
      "          (conv): Sequential(\n",
      "            (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=same, dilation=(2, 2), bias=False)\n",
      "            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU(inplace=True)\n",
      "            (3): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=same, dilation=(2, 2), bias=False)\n",
      "            (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (5): ReLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (1): DoubleConv(\n",
      "          (conv): Sequential(\n",
      "            (0): Conv2d(128, 128, kernel_size=(5, 5), stride=(1, 1), padding=same, dilation=(4, 4), bias=False)\n",
      "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU(inplace=True)\n",
      "            (3): Conv2d(128, 128, kernel_size=(5, 5), stride=(1, 1), padding=same, dilation=(4, 4), bias=False)\n",
      "            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (5): ReLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (2): DoubleConv(\n",
      "          (conv): Sequential(\n",
      "            (0): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=same, dilation=(8, 8), bias=False)\n",
      "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU(inplace=True)\n",
      "            (3): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=same, dilation=(8, 8), bias=False)\n",
      "            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (5): ReLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (3): DoubleConv(\n",
      "          (conv): Sequential(\n",
      "            (0): Conv2d(512, 512, kernel_size=(5, 5), stride=(1, 1), padding=same, dilation=(16, 16), bias=False)\n",
      "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU(inplace=True)\n",
      "            (3): Conv2d(512, 512, kernel_size=(5, 5), stride=(1, 1), padding=same, dilation=(16, 16), bias=False)\n",
      "            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (5): ReLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "  )\n",
      "  (final_conv): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uw4ztdEZQy3-",
    "outputId": "4038a571-8645-4d64-c1eb-295b5d91eae9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------+------------+\n",
      "|                  Modules                  | Parameters |\n",
      "+-------------------------------------------+------------+\n",
      "|      downConvs1.downs.0.conv.0.weight     |    1728    |\n",
      "|      downConvs1.downs.0.conv.1.weight     |     64     |\n",
      "|       downConvs1.downs.0.conv.1.bias      |     64     |\n",
      "|      downConvs1.downs.0.conv.3.weight     |   36864    |\n",
      "|      downConvs1.downs.0.conv.4.weight     |     64     |\n",
      "|       downConvs1.downs.0.conv.4.bias      |     64     |\n",
      "|      downConvs1.downs.1.conv.0.weight     |   73728    |\n",
      "|      downConvs1.downs.1.conv.1.weight     |    128     |\n",
      "|       downConvs1.downs.1.conv.1.bias      |    128     |\n",
      "|      downConvs1.downs.1.conv.3.weight     |   147456   |\n",
      "|      downConvs1.downs.1.conv.4.weight     |    128     |\n",
      "|       downConvs1.downs.1.conv.4.bias      |    128     |\n",
      "|      downConvs1.downs.2.conv.0.weight     |   294912   |\n",
      "|      downConvs1.downs.2.conv.1.weight     |    256     |\n",
      "|       downConvs1.downs.2.conv.1.bias      |    256     |\n",
      "|      downConvs1.downs.2.conv.3.weight     |   589824   |\n",
      "|      downConvs1.downs.2.conv.4.weight     |    256     |\n",
      "|       downConvs1.downs.2.conv.4.bias      |    256     |\n",
      "|      downConvs1.downs.3.conv.0.weight     |  1179648   |\n",
      "|      downConvs1.downs.3.conv.1.weight     |    512     |\n",
      "|       downConvs1.downs.3.conv.1.bias      |    512     |\n",
      "|      downConvs1.downs.3.conv.3.weight     |  2359296   |\n",
      "|      downConvs1.downs.3.conv.4.weight     |    512     |\n",
      "|       downConvs1.downs.3.conv.4.bias      |    512     |\n",
      "| downConvs1.residualBlocks.0.conv.0.weight |   102400   |\n",
      "| downConvs1.residualBlocks.0.conv.1.weight |     64     |\n",
      "|  downConvs1.residualBlocks.0.conv.1.bias  |     64     |\n",
      "| downConvs1.residualBlocks.0.conv.3.weight |   102400   |\n",
      "| downConvs1.residualBlocks.0.conv.4.weight |     64     |\n",
      "|  downConvs1.residualBlocks.0.conv.4.bias  |     64     |\n",
      "| downConvs1.residualBlocks.1.conv.0.weight |   409600   |\n",
      "| downConvs1.residualBlocks.1.conv.1.weight |    128     |\n",
      "|  downConvs1.residualBlocks.1.conv.1.bias  |    128     |\n",
      "| downConvs1.residualBlocks.1.conv.3.weight |   409600   |\n",
      "| downConvs1.residualBlocks.1.conv.4.weight |    128     |\n",
      "|  downConvs1.residualBlocks.1.conv.4.bias  |    128     |\n",
      "| downConvs1.residualBlocks.2.conv.0.weight |  1638400   |\n",
      "| downConvs1.residualBlocks.2.conv.1.weight |    256     |\n",
      "|  downConvs1.residualBlocks.2.conv.1.bias  |    256     |\n",
      "| downConvs1.residualBlocks.2.conv.3.weight |  1638400   |\n",
      "| downConvs1.residualBlocks.2.conv.4.weight |    256     |\n",
      "|  downConvs1.residualBlocks.2.conv.4.bias  |    256     |\n",
      "| downConvs1.residualBlocks.3.conv.0.weight |  6553600   |\n",
      "| downConvs1.residualBlocks.3.conv.1.weight |    512     |\n",
      "|  downConvs1.residualBlocks.3.conv.1.bias  |    512     |\n",
      "| downConvs1.residualBlocks.3.conv.3.weight |  6553600   |\n",
      "| downConvs1.residualBlocks.3.conv.4.weight |    512     |\n",
      "|  downConvs1.residualBlocks.3.conv.4.bias  |    512     |\n",
      "|      downConvs2.downs.0.conv.0.weight     |    1152    |\n",
      "|      downConvs2.downs.0.conv.1.weight     |     64     |\n",
      "|       downConvs2.downs.0.conv.1.bias      |     64     |\n",
      "|      downConvs2.downs.0.conv.3.weight     |   36864    |\n",
      "|      downConvs2.downs.0.conv.4.weight     |     64     |\n",
      "|       downConvs2.downs.0.conv.4.bias      |     64     |\n",
      "|      downConvs2.downs.1.conv.0.weight     |   73728    |\n",
      "|      downConvs2.downs.1.conv.1.weight     |    128     |\n",
      "|       downConvs2.downs.1.conv.1.bias      |    128     |\n",
      "|      downConvs2.downs.1.conv.3.weight     |   147456   |\n",
      "|      downConvs2.downs.1.conv.4.weight     |    128     |\n",
      "|       downConvs2.downs.1.conv.4.bias      |    128     |\n",
      "|      downConvs2.downs.2.conv.0.weight     |   294912   |\n",
      "|      downConvs2.downs.2.conv.1.weight     |    256     |\n",
      "|       downConvs2.downs.2.conv.1.bias      |    256     |\n",
      "|      downConvs2.downs.2.conv.3.weight     |   589824   |\n",
      "|      downConvs2.downs.2.conv.4.weight     |    256     |\n",
      "|       downConvs2.downs.2.conv.4.bias      |    256     |\n",
      "|      downConvs2.downs.3.conv.0.weight     |  1179648   |\n",
      "|      downConvs2.downs.3.conv.1.weight     |    512     |\n",
      "|       downConvs2.downs.3.conv.1.bias      |    512     |\n",
      "|      downConvs2.downs.3.conv.3.weight     |  2359296   |\n",
      "|      downConvs2.downs.3.conv.4.weight     |    512     |\n",
      "|       downConvs2.downs.3.conv.4.bias      |    512     |\n",
      "| downConvs2.residualBlocks.0.conv.0.weight |   102400   |\n",
      "| downConvs2.residualBlocks.0.conv.1.weight |     64     |\n",
      "|  downConvs2.residualBlocks.0.conv.1.bias  |     64     |\n",
      "| downConvs2.residualBlocks.0.conv.3.weight |   102400   |\n",
      "| downConvs2.residualBlocks.0.conv.4.weight |     64     |\n",
      "|  downConvs2.residualBlocks.0.conv.4.bias  |     64     |\n",
      "| downConvs2.residualBlocks.1.conv.0.weight |   409600   |\n",
      "| downConvs2.residualBlocks.1.conv.1.weight |    128     |\n",
      "|  downConvs2.residualBlocks.1.conv.1.bias  |    128     |\n",
      "| downConvs2.residualBlocks.1.conv.3.weight |   409600   |\n",
      "| downConvs2.residualBlocks.1.conv.4.weight |    128     |\n",
      "|  downConvs2.residualBlocks.1.conv.4.bias  |    128     |\n",
      "| downConvs2.residualBlocks.2.conv.0.weight |  1638400   |\n",
      "| downConvs2.residualBlocks.2.conv.1.weight |    256     |\n",
      "|  downConvs2.residualBlocks.2.conv.1.bias  |    256     |\n",
      "| downConvs2.residualBlocks.2.conv.3.weight |  1638400   |\n",
      "| downConvs2.residualBlocks.2.conv.4.weight |    256     |\n",
      "|  downConvs2.residualBlocks.2.conv.4.bias  |    256     |\n",
      "| downConvs2.residualBlocks.3.conv.0.weight |  6553600   |\n",
      "| downConvs2.residualBlocks.3.conv.1.weight |    512     |\n",
      "|  downConvs2.residualBlocks.3.conv.1.bias  |    512     |\n",
      "| downConvs2.residualBlocks.3.conv.3.weight |  6553600   |\n",
      "| downConvs2.residualBlocks.3.conv.4.weight |    512     |\n",
      "|  downConvs2.residualBlocks.3.conv.4.bias  |    512     |\n",
      "|          bottleneck.conv.0.weight         |  4718592   |\n",
      "|          bottleneck.conv.1.weight         |    512     |\n",
      "|           bottleneck.conv.1.bias          |    512     |\n",
      "|          bottleneck.conv.3.weight         |  2359296   |\n",
      "|          bottleneck.conv.4.weight         |    512     |\n",
      "|           bottleneck.conv.4.bias          |    512     |\n",
      "|            upConvs.ups.0.weight           |  1048576   |\n",
      "|             upConvs.ups.0.bias            |    512     |\n",
      "|        upConvs.ups.1.conv.0.weight        |  3538944   |\n",
      "|        upConvs.ups.1.conv.1.weight        |    256     |\n",
      "|         upConvs.ups.1.conv.1.bias         |    256     |\n",
      "|        upConvs.ups.1.conv.3.weight        |   589824   |\n",
      "|        upConvs.ups.1.conv.4.weight        |    256     |\n",
      "|         upConvs.ups.1.conv.4.bias         |    256     |\n",
      "|            upConvs.ups.2.weight           |   262144   |\n",
      "|             upConvs.ups.2.bias            |    256     |\n",
      "|        upConvs.ups.3.conv.0.weight        |   884736   |\n",
      "|        upConvs.ups.3.conv.1.weight        |    128     |\n",
      "|         upConvs.ups.3.conv.1.bias         |    128     |\n",
      "|        upConvs.ups.3.conv.3.weight        |   147456   |\n",
      "|        upConvs.ups.3.conv.4.weight        |    128     |\n",
      "|         upConvs.ups.3.conv.4.bias         |    128     |\n",
      "|            upConvs.ups.4.weight           |   65536    |\n",
      "|             upConvs.ups.4.bias            |    128     |\n",
      "|        upConvs.ups.5.conv.0.weight        |   221184   |\n",
      "|        upConvs.ups.5.conv.1.weight        |     64     |\n",
      "|         upConvs.ups.5.conv.1.bias         |     64     |\n",
      "|        upConvs.ups.5.conv.3.weight        |   36864    |\n",
      "|        upConvs.ups.5.conv.4.weight        |     64     |\n",
      "|         upConvs.ups.5.conv.4.bias         |     64     |\n",
      "|            upConvs.ups.6.weight           |   16384    |\n",
      "|             upConvs.ups.6.bias            |     64     |\n",
      "|        upConvs.ups.7.conv.0.weight        |   55296    |\n",
      "|        upConvs.ups.7.conv.1.weight        |     32     |\n",
      "|         upConvs.ups.7.conv.1.bias         |     32     |\n",
      "|        upConvs.ups.7.conv.3.weight        |    9216    |\n",
      "|        upConvs.ups.7.conv.4.weight        |     32     |\n",
      "|         upConvs.ups.7.conv.4.bias         |     32     |\n",
      "|             final_conv.weight             |     32     |\n",
      "|              final_conv.bias              |     1      |\n",
      "+-------------------------------------------+------------+\n",
      "Total Trainable Params: 58156705\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "58156705"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        param = parameter.numel()\n",
    "        table.add_row([name, param])\n",
    "        total_params+=param\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params\n",
    "    \n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "24-hoc8fVYUG"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Y Net with New Conv block.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
